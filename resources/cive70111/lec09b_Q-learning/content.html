<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Model-Free RL</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="../phoebe-js/phoebe.css">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="iframe-content">
        <div class="container">
            <!-- InfoTab (left sidebar) -->
            <div class="infotab">
                <div class="infotab-header"></div>
                <div class="infotab-content">
                    <div class="infotab-panel active" data-tab-title="Introduction">
                        <h3>Comparing Model-Free RL Algorithms</h3>
                        <p>This demonstration compares three fundamental reinforcement learning algorithms on the CartPole balancing task:</p>
                        <ul>
                            <li><strong>Monte Carlo (MC)</strong>: Updates Q-values at episode end using the full episode return</li>
                            <li><strong>SARSA</strong>: On-policy temporal difference learning using the actual next action taken</li>
                            <li><strong>Q-learning</strong>: Off-policy temporal difference learning using the maximum Q-value over next actions</li>
                        </ul>
                        <p>All three agents use tabular Q-tables with discretized states. You can switch between training checkpoints to see how each algorithm learns over time.</p>
                        <p><strong>Note:</strong> Unlike the CartPole demo, episodes here terminate immediately when the pole angle or cart position exceeds the bounds, matching the standard Gymnasium environment behavior.</p>
                        <h4>Key Concepts</h4>
                        <ul>
                            <li><strong>On-policy vs Off-policy</strong>: SARSA learns from actions it actually takes, while Q-learning learns from optimal actions</li>
                            <li><strong>Temporal Difference</strong>: TD methods (SARSA, Q-learning) update after each step, while MC waits until episode end</li>
                            <li><strong>Exploration vs Exploitation</strong>: All algorithms use epsilon-greedy policies with decaying exploration</li>
                        </ul>
                    </div>

                    <div class="infotab-panel" data-tab-title="Theory">
                        <h3>Model-Free Control Refresher</h3>
                        <p>All agents maximize the same discounted return but bootstrap differently:</p>
                        <ul>
                            <li><strong>Monte Carlo</strong>: wait for the full episode return \(G_t\) and update \(Q \leftarrow Q + \alpha (G_t - Q)\). Low bias, high variance.</li>
                            <li><strong>SARSA</strong>: on-policy target \(R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})\). Learns cautious policies that reflect the exploration behavior.</li>
                            <li><strong>Q-learning</strong>: off-policy target \(R_{t+1} + \gamma \max_a Q(S_{t+1}, a)\). Fast convergence but can overestimate with noisy values.</li>
                        </ul>
                        <p>Each agent uses epsilon-greedy exploration. SARSA and Q-learning now decay \(\epsilon\) per <em>environment step</em> so TD methods see the same exploration schedule regardless of episode length, while Monte Carlo continues to decay after each episode.</p>
                        <h4>State Discretization</h4>
                        <p>CartPole’s continuous state \((x, \dot{x}, \theta, \dot{\theta})\) is bucketed into 5 bins per dimension (625 states) with 2 discrete actions. The resulting Q-table stores 1,250 values per checkpoint.</p>
                        <table class="info-table">
                            <tr>
                                <th>Variable</th>
                                <th>Range</th>
                                <th>Bins</th>
                            </tr>
                            <tr>
                                <td>Cart Position (x)</td>
                                <td>[-2.4, 2.4] m</td>
                                <td>5 bins</td>
                            </tr>
                            <tr>
                                <td>Cart Velocity (ẋ)</td>
                                <td>[-3, 3] m/s</td>
                                <td>5 bins</td>
                            </tr>
                            <tr>
                                <td>Pole Angle (θ)</td>
                                <td>[-0.21, 0.21] rad</td>
                                <td>5 bins</td>
                            </tr>
                            <tr>
                                <td>Angular Velocity (θ̇)</td>
                                <td>[-2, 2] rad/s</td>
                                <td>5 bins</td>
                            </tr>
                        </table>
                        <p><strong>Trade-off:</strong> coarse bins speed up learning but miss nuance; finer bins explode the state space.</p>
                    </div>

                    <div class="infotab-panel" data-tab-title="Tips">
                        <h3>Why MC Looks Strong Here</h3>
                        <ul>
                            <li><strong>Episodic fit:</strong> CartPole is short-horizon and resets often, so using full-episode returns matches the problem well and avoids bootstrapping bias.</li>
                            <li><strong>Coarse bins:</strong> With 5×5×5×5 discretization, Q-values are noisy; MC’s unbiased targets handle this noise better than TD’s max over rough estimates.</li>
                            <li><strong>Exploration schedule:</strong> MC decays \(\epsilon\) per episode, so it turns greedy sooner than the TD agents (which decay per step), lifting its logged returns earlier.</li>
                            <li><strong>On-policy stability:</strong> SARSA learns the behavior policy and can be cautious; Q-learning’s off-policy max can overestimate. MC sidesteps both issues.</li>
                            <li><strong>Why the curve looks jagged:</strong> In RL you’re averaging over entire episodes, not i.i.d. samples. Returns swing with stochastic starts and exploration, so curves are much noisier than supervised loss plots. The practical signal is the trend and the greedy-eval marker—not the small bumps.</li>
                        </ul>
                    </div>

                    <div class="infotab-panel" data-tab-title="Instructions">
                        <h3>How to Use This Demo</h3>
                        <ul>
                            <li><strong>Select a checkpoint</strong> (0, 10k, 80k, 150k steps) to load the saved Q-table and statistics for all three algorithms.</li>
                            <li><strong>Play / Step / Reset</strong> controls run the current agent policy inside the browser CartPole environment. Reset also stops playback.</li>
                            <li><strong>Learning curve panel</strong> plots return vs. training steps (rescaled from logged episodes). Use it to compare sample efficiency.</li>
                            <li><strong>Q-table heatmap</strong> shows a slice through \(x=0, \dot{x}=0\) with the greedy action arrow. Hover between checkpoints to see how policies sharpen.</li>
                            <li><strong>Metrics row</strong> shows training average return, greedy-policy average return (evaluated in-browser), and the exploration rate at that checkpoint.</li>
                            <li><strong>Checkpoint notes</strong> under the buttons explain what changes between training stages—read them while flipping between algorithms.</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- Main demo area -->
            <div class="demo-wrapper">
                <!-- Checkpoint Selector (global controls) -->
                <div class="checkpoint-controls">
                    <h3>Training Checkpoint</h3>
                    <div class="checkpoint-selector">
                        <button class="checkpoint-btn active" data-step="0">0 steps<br>(initial)</button>
                        <button class="checkpoint-btn" data-step="10000">10k steps<br>(early)</button>
                        <button class="checkpoint-btn" data-step="80000">80k steps<br>(mid)</button>
                        <button class="checkpoint-btn" data-step="150000">150k steps<br>(converged)</button>
                    </div>
                    <div class="teaching-callout" id="teaching-callout">
                        <strong id="current-config">Currently viewing: Random initialization (0 steps)</strong>
                        <p id="teaching-tip">Random initialization - watch all three agents fail to balance!</p>
                    </div>
                </div>

                <!-- Three-column comparison -->
                <div class="algorithms-grid">
                    <!-- Monte Carlo Column -->
                    <div class="algorithm-column" id="mc-column">
                        <div class="algo-header">
                            <h3>Monte Carlo</h3>
                            <span class="algo-badge">Episode-end updates</span>
                        </div>

                        <div class="canvas-card">
                            <div class="canvas-shell">
                                <span class="attempt-counter" id="mc-attempt">Attempt: 01</span>
                                <canvas id="mc-canvas" width="400" height="200"></canvas>
                            </div>
                        </div>

                        <div class="metrics-row">
                            <div class="metriclabel" id="mc-avg-return"></div>
                            <div class="metriclabel" id="mc-greedy-return"></div>
                            <div class="metriclabel" id="mc-epsilon"></div>
                        </div>

                        <div class="chart-card">
                            <h4>Learning Curve</h4>
                            <canvas id="mc-chart" height="150"></canvas>
                        </div>

                        <div class="qtable-card">
                            <h4>Q-Table (θ vs θ̇)</h4>
                            <canvas id="mc-qtable" width="250" height="250"></canvas>
                        </div>
                    </div>

                    <!-- SARSA Column -->
                    <div class="algorithm-column" id="sarsa-column">
                        <div class="algo-header">
                            <h3>SARSA</h3>
                            <span class="algo-badge">On-policy TD</span>
                        </div>

                        <div class="canvas-card">
                            <div class="canvas-shell">
                                <span class="attempt-counter" id="sarsa-attempt">Attempt: 01</span>
                                <canvas id="sarsa-canvas" width="400" height="200"></canvas>
                            </div>
                        </div>

                        <div class="metrics-row">
                            <div class="metriclabel" id="sarsa-avg-return"></div>
                            <div class="metriclabel" id="sarsa-greedy-return"></div>
                            <div class="metriclabel" id="sarsa-epsilon"></div>
                        </div>

                        <div class="chart-card">
                            <h4>Learning Curve</h4>
                            <canvas id="sarsa-chart" height="150"></canvas>
                        </div>

                        <div class="qtable-card">
                            <h4>Q-Table (θ vs θ̇)</h4>
                            <canvas id="sarsa-qtable" width="250" height="250"></canvas>
                        </div>
                    </div>

                    <!-- Q-learning Column -->
                    <div class="algorithm-column" id="qlearning-column">
                        <div class="algo-header">
                            <h3>Q-learning</h3>
                            <span class="algo-badge">Off-policy TD</span>
                        </div>

                        <div class="canvas-card">
                            <div class="canvas-shell">
                                <span class="attempt-counter" id="qlearning-attempt">Attempt: 01</span>
                                <canvas id="qlearning-canvas" width="400" height="200"></canvas>
                            </div>
                        </div>

                        <div class="metrics-row">
                            <div class="metriclabel" id="qlearning-avg-return"></div>
                            <div class="metriclabel" id="qlearning-greedy-return"></div>
                            <div class="metriclabel" id="qlearning-epsilon"></div>
                        </div>

                        <div class="chart-card">
                            <h4>Learning Curve</h4>
                            <canvas id="qlearning-chart" height="150"></canvas>
                        </div>

                        <div class="qtable-card">
                            <h4>Q-Table (θ vs θ̇)</h4>
                            <canvas id="qlearning-qtable" width="250" height="250"></canvas>
                        </div>
                    </div>
                </div>

                <!-- Shared TD Error Display -->
                <div class="td-error-panel">
                    <h3>Temporal Difference Error Breakdown</h3>
                    <p class="td-subtitle">Click an agent to see its update rule with sample values</p>
                    <div id="td-breakdown">
                        <p>Select a checkpoint and click "Play" on an agent to see TD error details.</p>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- Scripts -->
    <script src="../phoebe-js/canvashelper.js"></script>
    <script src="../phoebe-js/metriclabel.js"></script>
    <script src="../phoebe-js/infotab.js"></script>
    <script src="cartpole-engine.js?v=2"></script>
    <script src="q-learning-engine.js?v=2"></script>
    <script src="snapshot-loader.js?v=2"></script>
    <script src="q-table-viz.js?v=2"></script>
    <script src="script.js?v=2"></script>
</body>
</html>
