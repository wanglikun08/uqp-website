<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Higher-Order Features</title>
    <script src="../scripts/mathjax-config.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.js"></script>
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="../scripts/demotab.css">
    <link rel="stylesheet" href="../demo-common.css">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="iframe-content">
        <div class="container">

        <!-- Info Tabs -->
        <div class="info-tabs">
            <div class="tab-header">
            </div>

            <div class="tab-content">
                <div class="tab-panel active" data-tab-title="Introduction">
                    The bias-variance tradeoff is fundamental to understanding model complexity in machine learning. When fitting a polynomial to data, we face a critical choice: simple models (low degree) may underfit, while complex models (high degree) may overfit.<br><br>

                    This demo uses synthetic data (12 training points, 18 test points) with a complex underlying pattern combining polynomial and trigonometric components. You'll clearly see: degree 1 severely underfits (MSE ~8,500), degree 4 finds the sweet spot (MSE ~1,200), and degree 10 overfits dramatically (train MSE = 17, test MSE = 5,096!).
                </div>

                <div class="tab-panel" data-tab-title="Instructions">
                    ‚Ä¢ Adjust polynomial degree slider (1-10) to see how complexity affects train vs test error<br>
                    ‚Ä¢ Check "Show Squared Errors" to visualize MSE‚Äîarea of squares = prediction errors<br>
                    ‚Ä¢ Click "Toggle Test Data" to show/hide test points<br>
                    ‚Ä¢ Watch the error chart: Test MSE drops sharply, then explodes at degree 8+<br><br>

                    <strong>Why does degree 4 minimize test error? Why does train error reach 0 but test error explodes?</strong>
                </div>

                <div class="tab-panel" data-tab-title="Tips">
                    ‚Ä¢ <strong>Degree 1:</strong> Straight line‚Äîsevere underfitting (MSE ~8,500)<br>
                    ‚Ä¢ <strong>Degree 2-3:</strong> Still underfitting (MSE ~2,900 ‚Üí ~260)<br>
                    ‚Ä¢ <strong>Degree 4:</strong> ‚úì SWEET SPOT (Test MSE ~1,200)‚Äîbest generalization!<br>
                    ‚Ä¢ <strong>Degrees 5-8:</strong> Slight overfitting (test MSE ~1,200-1,400)<br>
                    ‚Ä¢ <strong>Degrees 9-10:</strong> Clear overfitting‚Äîtrain MSE drops to 17, test MSE rises to ~5,000!<br>
                    ‚Ä¢ <strong>Balanced dataset:</strong> 12 training, 18 test points show realistic bias-variance progression
                </div>
            </div>
        </div>

        <!-- Demo Area -->
        <div class="demo-area">
            <!-- Chart: Scatter Plot with Polynomial Fit -->
            <div class="plot-container">
                <canvas id="scatterPlot" style="max-width: 650px; max-height: 520px;"></canvas>
            </div>

            <!-- Controls -->
            <div class="controls">
                <div class="control-group">
                    <label for="degree-slider"><strong>Polynomial Degree:</strong></label>
                    <div class="slider-container">
                        <input type="range" id="degree-slider" min="1" max="10" step="1" value="1">
                        <div class="value-display" id="degree-value">1</div>
                    </div>
                </div>

                <div class="equation" id="equation">
                    $y = \beta_0 + \beta_1 x$
                </div>

                <div class="control-group">
                    <label><strong>Display Options:</strong></label>
                    <div class="feature-grid">
                        <div class="feature-box feature-squared-errors" id="toggle-squared-errors">
                            <div class="feature-icon">üìê</div>
                            <div class="feature-label">Squared Errors</div>
                        </div>
                        <div class="feature-box feature-test-data" id="toggle-test-data">
                            <div class="feature-icon">üî¨</div>
                            <div class="feature-label">Test Data</div>
                        </div>
                    </div>
                </div>

                <div class="equation" id="train-mse">
                    MSE = 0.0
                </div>

                <div class="equation" id="test-mse">
                    MSE = 0.0
                </div>
            </div>
        </div>

        <!-- Chart: Error Decomposition (full width below) -->
        <div class="plot-container" style="margin-top: 30px;">
            <div style="position: relative; width: 100%; height: 300px;">
                <canvas id="errorChart"></canvas>
            </div>
        </div>

        <!-- Theory Section -->
        <div class="theory-section" style="display: none;">
            <h3>Mathematical Foundations</h3>
            <p>
                <strong>Polynomial Regression</strong> models the relationship between temperature $x$ and bike rentals $y$ using a polynomial of degree $d$:
                $$\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_d x^d$$<br>

                Increasing $d$ allows more flexible curves but risks overfitting to noise.<br><br>

                <strong>Bias-Variance Decomposition</strong> breaks prediction error into three components:
                $$\text{Expected Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}$$<br>

                <strong>Bias¬≤</strong> measures error from wrong model assumptions. High-bias models (low degree polynomials) are too simple to capture the true pattern‚Äîthey systematically underpredict or overpredict. As model complexity increases, bias decreases.<br><br>

                <strong>Variance</strong> measures sensitivity to specific training data. High-variance models (high degree polynomials) fit training noise and vary wildly when trained on different samples. As model complexity increases, variance increases.<br><br>

                <strong>Irreducible Error</strong> is noise inherent in the data that no model can eliminate (weather variations, individual preferences, measurement errors).<br><br>

                <strong>The Tradeoff:</strong> Optimal model complexity balances bias and variance. Too simple = high bias (underfitting). Too complex = high variance (overfitting). The sweet spot minimizes total error on unseen test data.<br><br>

                <strong>Training vs Test Error:</strong> Training error always decreases with complexity (more parameters = better fit to training data). Test error follows a U-shaped curve: decreases initially (reducing bias), reaches minimum at optimal complexity, then increases (variance dominates). The gap between train and test error indicates overfitting‚Äîlarger gap means the model memorized training-specific patterns.
            </p>
        </div>

        <p style="font-style: italic; text-align: right; margin-top: 10px; color: #666; font-size: 14px;">
            Developed by <a href="https://transport-systems.imperial.ac.uk/members/qu-k" target="_blank" style="color: #666; text-decoration: underline;">Kevin Yu</a> & <a href="https://transport-systems.imperial.ac.uk/members/angeloudis-p" target="_blank" style="color: #666; text-decoration: underline;">Panagiotis Angeloudis</a>
        </p>

    </div>
    </div>

    <script src="../scripts/demotab.js"></script>
    <script src="../scripts/metric-box-display.js"></script>
    <script src="../scripts/slider-control.js"></script>
    <script src="script.js"></script>
</body>
</html>
