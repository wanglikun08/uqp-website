class MultiArmedBandit{constructor(customDistributions=null){this.numArms=3,this.armDistributions=customDistributions||this.setupDefaultArms(),this.trueMeans=this.calculateTrueMeans()}setupDefaultArms(){return[{rewards:[-1,1],probabilities:[.5,.5]},{rewards:[1,0],probabilities:[.4,.6]},{rewards:[2,-1,0],probabilities:[.3,.1,.6]}]}calculateTrueMeans(){return this.armDistributions.map(arm=>arm.rewards.reduce((sum,reward,i)=>sum+reward*arm.probabilities[i],0))}pullArm(armIndex){if(armIndex<0||armIndex>=this.numArms)throw new Error(`Invalid arm index: ${armIndex}. Must be 0, 1, or 2.`);const arm=this.armDistributions[armIndex],random=Math.random();let cumulativeProb=0;for(let i=0;i<arm.probabilities.length;i++)if(cumulativeProb+=arm.probabilities[i],random<=cumulativeProb)return arm.rewards[i];return arm.rewards[arm.rewards.length-1]}getArmInfo(armIndex){if(armIndex<0||armIndex>=this.numArms)throw new Error(`Invalid arm index: ${armIndex}`);return{armIndex:armIndex,rewards:this.armDistributions[armIndex].rewards,probabilities:this.armDistributions[armIndex].probabilities,trueMean:this.trueMeans[armIndex]}}getAllArmsInfo(){return this.armDistributions.map((_,i)=>this.getArmInfo(i))}getOptimalArm(){return this.trueMeans.indexOf(Math.max(...this.trueMeans))}}class BanditSession{constructor(maxPulls=50,customDistributions=null){this.bandit=new MultiArmedBandit(customDistributions),this.maxPulls=maxPulls,this.reset()}reset(){this.totalPulls=0,this.armPulls=[0,0,0],this.armRewards=[0,0,0],this.pullHistory=[]}pullArm(armIndex){const armIdx=armIndex-1;if(this.totalPulls>=this.maxPulls)return{success:!1,message:`Maximum pulls (${this.maxPulls}) reached!`};try{const reward=this.bandit.pullArm(armIdx);return this.totalPulls++,this.armPulls[armIdx]++,this.armRewards[armIdx]+=reward,this.pullHistory.push({arm:armIndex,reward:reward}),{success:!0,reward:reward,arm:armIndex,statistics:this.getStatistics()}}catch(error){return{success:!1,message:error.message}}}getStatistics(){const armMeans=this.armPulls.map((pulls,i)=>pulls>0?Math.round(this.armRewards[i]/pulls*1e3)/1e3:0);return{total_pulls:this.totalPulls,max_pulls:this.maxPulls,arm_pulls:this.armPulls,arm_rewards:this.armRewards,arm_means:armMeans,pull_history:this.pullHistory.slice(-10)}}canPull(){return this.totalPulls<this.maxPulls}}class EpsilonGreedyAgent{constructor(numArms=3,epsilon=.1){this.numArms=numArms,this.epsilon=epsilon,this.reset()}reset(){this.armCounts=new Array(this.numArms).fill(0),this.armValues=new Array(this.numArms).fill(0),this.totalPulls=0,this.totalReward=0,this.pullHistory=[]}selectArm(){if(Math.random()<this.epsilon){return{arm:Math.floor(Math.random()*this.numArms),actionType:"explore"}}return{arm:this.armValues.indexOf(Math.max(...this.armValues)),actionType:"exploit"}}update(arm,reward){this.armCounts[arm]++,this.armValues[arm]+=(reward-this.armValues[arm])/this.armCounts[arm],this.totalPulls++,this.totalReward+=reward,this.pullHistory.push({arm:arm,reward:reward,armValues:[...this.armValues],armCounts:[...this.armCounts]})}getEstimates(){return{arm_estimates:[...this.armValues],arm_counts:[...this.armCounts],total_reward:this.totalReward,average_reward:this.totalPulls>0?this.totalReward/this.totalPulls:0,epsilon:this.epsilon}}}class BanditSimulator{constructor(){this.bandit=new MultiArmedBandit,this.banditData=null}async loadBanditData(){if(this.banditData)return this.banditData;try{const response=await fetch("./data/bandit_data.json");if(!response.ok)throw new Error(`HTTP error! status: ${response.status}`);return this.banditData=await response.json(),console.log("Bandit data loaded successfully"),this.banditData}catch(error){throw console.error("Failed to load bandit data:",error),error}}async demonstrateExploration(numSteps=100){await this.loadBanditData();const explorationData=this.banditData.exploration_simulation;return{simulation_steps:explorationData.simulation_steps.map(step=>({step:step.step,arm:step.arm,action_type:step.action_type,reward:step.reward,estimates:step.arm_values_after||[0,0,0],counts:step.arm_counts_after||[0,0,0],total_reward:step.total_reward||0})),summary:{total_reward:explorationData.summary.total_reward,average_reward:explorationData.summary.average_reward,epsilon:explorationData.summary.epsilon,final_estimates:explorationData.summary.final_arm_values}}}async demonstrateTrained(numSteps=100){await this.loadBanditData();const trainedData=this.banditData.trained_simulation;return{simulation_steps:trainedData.simulation_steps.map(step=>({step:step.step,arm:step.arm,action_type:step.action_type,reward:step.reward,estimates:step.arm_values,confidence:step.confidence})),summary:{optimal_arm_selections:trainedData.summary.optimal_arm_selections,optimal_percentage:trainedData.summary.optimal_percentage,epsilon:trainedData.summary.epsilon,initial_estimates:trainedData.summary.initial_arm_values}}}async revealDistributions(){await this.loadBanditData();const environment=this.banditData.environment;return{success:!0,arms_info:environment.arms.map(arm=>({arm:arm.arm_index+1,rewards:arm.rewards,probabilities:arm.probabilities,expected_reward:arm.true_mean})),optimal_arm:environment.optimal_arm+1,true_means:environment.true_means}}simulateExplorationStep(agent,step){const selection=agent.selectArm(),arm=selection.arm,actionType=selection.actionType,reward=this.bandit.pullArm(arm),stepData={step:step,arm:arm,action_type:actionType,reward:reward,arm_values_before:[...agent.armValues],arm_counts_before:[...agent.armCounts]};return agent.update(arm,reward),stepData.arm_values_after=[...agent.armValues],stepData.arm_counts_after=[...agent.armCounts],stepData.total_reward=agent.totalReward,stepData.average_reward=agent.totalReward/(step+1),stepData}}